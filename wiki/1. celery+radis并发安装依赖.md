要将你的代码改写成基于 Django 框架的并发版本，同时考虑到 OpenAI 的 Python 客户端不支持异步，我们可以使用 **Celery** 来处理后台任务。以下是详细的改写步骤：

------

### **1. 安装依赖**

首先，安装 Celery 和 Redis（作为消息队列）：

bash

复制

```
pip install celery redis
```

------

### **2. 配置 Celery**

在 Django 项目中创建 `celery.py` 文件，配置 Celery 并定义任务：

python

复制

```
import os
from celery import Celery
from openai import OpenAI

# 设置 Django 的默认设置模块
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'your_project.settings')

# 创建 Celery 应用
app = Celery('your_project')

# 从 Django 配置中加载 Celery 设置
app.config_from_object('django.conf:settings', namespace='CELERY')

# 自动发现任务
app.autodiscover_tasks()

# 初始化 OpenAI 客户端
client = OpenAI(
    api_key=os.environ.get("APK_API_KEY_huoshan"),
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    timeout=1800
)

# 定义 Celery 任务
@app.task
def process_chat(message):
    try:
        print("-----streaming request-----")
        print("发送消息：", message[1]["content"])

        # 调用 API 并启用流式响应
        stream = client.chat.completions.create(
            model="deepseek-r1-250120",  # 替换为实际模型名称
            messages=message,
            stream=True
        )

        # 处理流式响应
        full_response = ""
        for chunk in stream:
            if not chunk.choices:
                continue
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)
            full_response += content

        # 将响应添加到消息历史中
        message.append({"role": "assistant", "content": full_response})
        print("\n收到响应：", full_response)

        return full_response

    except Exception as e:
        print("Error:", e)
        return str(e)
```

------

### **3. 改写 Django 视图**

在 Django 视图中调用 Celery 任务：

python

复制

```
from django.http import JsonResponse
from django.views import View
from .tasks import process_chat

class ChatView(View):
    def post(self, request):
        # 初始化消息
        message = [
            {"role": "system", "content": "你是人工智能助手"},
            {"role": "user", "content": "今天有哪些新闻"}
        ]

        # 调用 Celery 任务
        task = process_chat.delay(message)
        return JsonResponse({"task_id": task.id})
```

------

### **4. 配置 URL**

在 `urls.py` 中配置路由：

python

复制

```
from django.urls import path
from .views import ChatView

urlpatterns = [
    path('chat/', ChatView.as_view(), name='chat'),
]
```

------

### **5. 启动 Celery Worker**

启动 Celery Worker 处理任务：

bash

复制

```
celery -A your_project worker --loglevel=info
```

------

### **6. 运行 Django 服务器**

启动 Django 服务器：

bash

复制

```
python manage.py runserver
```

------

### **7. 测试**

发送 POST 请求到 `/chat/` 端点，查看任务 ID，并通过 Celery 监控任务状态。

------

### **总结**

- 通过 Celery 将 OpenAI 的同步任务放入后台执行，避免阻塞 Django 的主线程。
- 这种方法适合处理耗时较长的任务，同时保持 Django 的响应性能。